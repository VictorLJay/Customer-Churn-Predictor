{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# ML classifier models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter\n",
    "\n",
    "# model selection (CV)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the DF into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Dependent_count</th>\n",
       "      <th>Months_on_book</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Credit_Limit</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>Total_Trans_Amt</th>\n",
       "      <th>...</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "      <th>Education_Level_encoded</th>\n",
       "      <th>Income_Category_encoded</th>\n",
       "      <th>Card_Category_encoded</th>\n",
       "      <th>x0_Married</th>\n",
       "      <th>x0_Single</th>\n",
       "      <th>x0_Unknown</th>\n",
       "      <th>x1_Existing Customer</th>\n",
       "      <th>x2_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12691.0</td>\n",
       "      <td>777</td>\n",
       "      <td>1.335</td>\n",
       "      <td>1144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.061</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8256.0</td>\n",
       "      <td>864</td>\n",
       "      <td>1.541</td>\n",
       "      <td>1291</td>\n",
       "      <td>...</td>\n",
       "      <td>3.714</td>\n",
       "      <td>0.105</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.594</td>\n",
       "      <td>1887</td>\n",
       "      <td>...</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3313.0</td>\n",
       "      <td>2517</td>\n",
       "      <td>1.405</td>\n",
       "      <td>1171</td>\n",
       "      <td>...</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0.760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4716.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.175</td>\n",
       "      <td>816</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age  Dependent_count  Months_on_book  Total_Relationship_Count  \\\n",
       "0            45                3              39                         5   \n",
       "1            49                5              44                         6   \n",
       "2            51                3              36                         4   \n",
       "3            40                4              34                         3   \n",
       "4            40                3              21                         5   \n",
       "\n",
       "   Months_Inactive_12_mon  Contacts_Count_12_mon  Credit_Limit  \\\n",
       "0                       1                      3       12691.0   \n",
       "1                       1                      2        8256.0   \n",
       "2                       1                      0        3418.0   \n",
       "3                       4                      1        3313.0   \n",
       "4                       1                      0        4716.0   \n",
       "\n",
       "   Total_Revolving_Bal  Total_Amt_Chng_Q4_Q1  Total_Trans_Amt  ...  \\\n",
       "0                  777                 1.335             1144  ...   \n",
       "1                  864                 1.541             1291  ...   \n",
       "2                    0                 2.594             1887  ...   \n",
       "3                 2517                 1.405             1171  ...   \n",
       "4                    0                 2.175              816  ...   \n",
       "\n",
       "   Total_Ct_Chng_Q4_Q1  Avg_Utilization_Ratio  Education_Level_encoded  \\\n",
       "0                1.625                  0.061                      2.0   \n",
       "1                3.714                  0.105                      4.0   \n",
       "2                2.333                  0.000                      4.0   \n",
       "3                2.333                  0.760                      2.0   \n",
       "4                2.500                  0.000                      1.0   \n",
       "\n",
       "   Income_Category_encoded  Card_Category_encoded  x0_Married  x0_Single  \\\n",
       "0                      3.0                    0.0         1.0        0.0   \n",
       "1                      1.0                    0.0         0.0        1.0   \n",
       "2                      4.0                    0.0         1.0        0.0   \n",
       "3                      1.0                    0.0         0.0        0.0   \n",
       "4                      3.0                    0.0         1.0        0.0   \n",
       "\n",
       "   x0_Unknown  x1_Existing Customer  x2_M  \n",
       "0         0.0                   1.0   1.0  \n",
       "1         0.0                   1.0   0.0  \n",
       "2         0.0                   1.0   1.0  \n",
       "3         1.0                   1.0   0.0  \n",
       "4         0.0                   1.0   1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = pd.read_csv(\"../data/bank_processed_data.csv\", index_col=0)\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating X and y\n",
    "X = bank.drop(columns=\"x1_Existing Customer\")\n",
    "y = bank[\"x1_Existing Customer\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "\n",
    "scaler = StandardScaler() # initialize the scaler\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column that I want to identify is **x1_Existing Customer**, being **1** if the customer is still a customer, and **0** otherwise. In this case, as it's a True/False decission, the models that fit better for this type of Supervised ML are the Classifiers.\n",
    "\n",
    "I will start checking the different models, without parameter tuning, for identify which are the models that perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the models\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "tree = DecisionTreeClassifier()\n",
    "gradient = GradientBoostingClassifier()\n",
    "RF = RandomForestClassifier()\n",
    "adaboost = AdaBoostClassifier()\n",
    "extra_tree = ExtraTreesClassifier()\n",
    "support_vector = SVC()\n",
    "\n",
    "\n",
    "models = [neigh, tree, RF, adaboost, gradient, extra_tree, support_vector]\n",
    "model_names = [\"KNeighbors\", \"DecisionTree\", \"RandomForest\", \"AdaBoost\", \n",
    "               \"GradientBoost\", \"ExtraTress\", \"SVC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data per each category is not balanced, as customers represent 83.8% of the sample, the accuracy here is not relevant. \n",
    "\n",
    "In this scenario, I will focus more on `recall`, to ensure that the model classifies correctly the labels, and the precision. Mention that, the main focus will be on the label **0.0, as it is the customers that already churned the bank, and we want to focus on that part** to ensure that our model is able to predict possible future cases and act before churn happens.\n",
    "\n",
    "Last, but not least, `macro avg` will also be taken into consideration, as we want to ensure that **0.0** are classified correctly, but we want that the amount of **1.0** are good too. I would have to find the perfect balance between those metrics.\n",
    "\n",
    "*NOTES*\n",
    "\n",
    "The `precision` is the ratio TP / (TP + FP) where TP is the number of true positives and FP the number of false positives. The precision is intuitively **the ability of the classifier not to label as positive a sample that is negative**.\n",
    "\n",
    "The `recall` is the ratio TP / (TP + FN) where TP is the number of true positives and FN the number of false negatives. The recall is intuitively **the ability of the classifier to find all the positive samples**. Note that in binary classification, recall of the positive class is also known as `sensitivity`; recall of the negative class is `specificity`.\n",
    "\n",
    "The most important for this work is to increase the **sensitivity** (to detect all churn cases). Even though the other parameters are very important also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top3_classifier_model(models):\n",
    "    \"\"\"\n",
    "    Input: Models to test\n",
    "    Output: DF top 3 models\n",
    "    \"\"\"\n",
    "    # first batch of empty lists    \n",
    "    time_to_train = []\n",
    "    accuracy = []\n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    macro_F1 = []\n",
    "    report_dict = []\n",
    "\n",
    "    for model in models:    \n",
    "        start = time.time()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # metrics\n",
    "        accuracy_ = accuracy_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "        precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        clasf_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # appending to empty lists\n",
    "        time_to_train.append((time.time() - start))\n",
    "        accuracy.append(round(accuracy_,4))\n",
    "        macro_precision.append(round(precision,4))\n",
    "        macro_recall.append(round(recall,4))\n",
    "        macro_F1.append(round(f1,4))\n",
    "        report_dict.append(clasf_report_dict)\n",
    "    \n",
    "    # second batch\n",
    "    precision_0 = []\n",
    "    recall_0 = []\n",
    "    f1_0 = []\n",
    "    precision_1 = []\n",
    "    recall_1 = []\n",
    "    f1_1 = []\n",
    "\n",
    "    for report in report_dict:\n",
    "        # Info of churn label\n",
    "        precision_0.append(round(report[\"0.0\"][\"precision\"],4))\n",
    "        recall_0.append(round(report[\"0.0\"][\"recall\"],4))\n",
    "        f1_0.append(round(report[\"0.0\"][\"f1-score\"],4))\n",
    "\n",
    "        # Info of current customers\n",
    "        precision_1.append(round(report[\"1.0\"][\"precision\"],4))\n",
    "        recall_1.append(round(report[\"1.0\"][\"recall\"],4))\n",
    "        f1_1.append(round(report[\"1.0\"][\"f1-score\"],4))\n",
    "        \n",
    "    # creating DF\n",
    "    best_models_DF = pd.DataFrame({\"model\":model_names,\n",
    "                                   \"training_time\":time_to_train,\n",
    "                                   \"accuracy\":accuracy,\n",
    "                                   \"precision_macro\":macro_precision,\n",
    "                                   \"recall_macro\":macro_recall,\n",
    "                                   \"f1_macro\":macro_F1,\n",
    "                                   \"precision_0\":precision_0,\n",
    "                                   \"recall_0\":recall_0,\n",
    "                                   \"f1_0\":f1_0,\n",
    "                                   \"precision_1\":precision_1,\n",
    "                                   \"recall_1\":recall_1,\n",
    "                                   \"f1_1\":f1_1\n",
    "                                  })\n",
    "    \n",
    "    # getting top 3 models\n",
    "    top3 = best_models_DF.sort_values(by=[\"f1_macro\"], ascending=False).reset_index(drop=True).iloc[:3]\n",
    "    \n",
    "    return top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>training_time</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>1.475824</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9194</td>\n",
       "      <td>0.9328</td>\n",
       "      <td>0.9233</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.9793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.866706</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9461</td>\n",
       "      <td>0.9132</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.9223</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.9781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.386940</td>\n",
       "      <td>0.9605</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>0.9180</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>0.8554</td>\n",
       "      <td>0.8742</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  training_time  accuracy  precision_macro  recall_macro  \\\n",
       "0  GradientBoost       1.475824    0.9650           0.9478        0.9194   \n",
       "1   RandomForest       0.866706    0.9630           0.9461        0.9132   \n",
       "2       AdaBoost       0.386940    0.9605           0.9332        0.9180   \n",
       "\n",
       "   f1_macro  precision_0  recall_0    f1_0  precision_1  recall_1    f1_1  \n",
       "0    0.9328       0.9233    0.8523  0.8864       0.9722    0.9865  0.9793  \n",
       "1    0.9287       0.9223    0.8400  0.8792       0.9699    0.9865  0.9781  \n",
       "2    0.9254       0.8939    0.8554  0.8742       0.9726    0.9806  0.9766  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_classifier_model(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three models that does the best selection for the churned customers and also for the actual customers are **GradientBoost**, **AdaBoost** and **RandomForest**.\n",
    "\n",
    "Now that we have in mind which models work best, let's start tuning them for improve their results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# gradient_params = {\"loss\":[\"deviance\", \"exponential\"],\n",
    "#                   \"criterion\":[\"friedman_mse\", \"mse\", \"mae\"],\n",
    "#                   \"max_features\":[\"auto\", \"sqrt\", \"log2\"],\n",
    "#                   \"n_estimators\":randint(low=50, high=300),\n",
    "#                   \"max_depth\":randint(low=2, high=8),\n",
    "#                   \"max_leaf_nodes\":randint(low=5, high=15)\n",
    "#                   }\n",
    "\n",
    "# scorers = {\"precision_score\": make_scorer(precision_score, average=\"macro\"),\n",
    "#            \"recall_score\": make_scorer(recall_score, average=\"macro\"),\n",
    "#            \"f1_score\": make_scorer(f1_score, average=\"macro\")\n",
    "#           }\n",
    "\n",
    "# gradient_search = RandomizedSearchCV(gradient,\n",
    "#                                      gradient_params,\n",
    "#                                      n_iter=10,\n",
    "#                                      n_jobs=-1,\n",
    "#                                      cv=StratifiedKFold(),\n",
    "#                                      scoring=scorers,\n",
    "#                                      refit=False,\n",
    "#                                      random_state=42\n",
    "#                                     )\n",
    "\n",
    "# gradient_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the results obtained, I will create a DF for visualize which are the scores for each scoring. After that, I will pick the parameters that performed better for passing it to GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_results = pd.DataFrame(gradient_search.cv_results_)\n",
    "\n",
    "# gradient_results = gradient_results[[\"params\", \"mean_test_precision_score\", \"rank_test_precision_score\",\n",
    "#                                      \"mean_test_recall_score\", \"rank_test_recall_score\",\n",
    "#                                      \"mean_test_f1_score\", \"rank_test_f1_score\"]]\n",
    "\n",
    "# gradient_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_results[\"params\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we know which are the best parameters to pass to the model, we will create a function for each one of them and then obtain their respective **y_pred**. The **y_value** obtained will be stored in a variable for, later on, build the `classification_report` and `confusion_matrix`.\n",
    "\n",
    "The structure will be the same for future models and variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_gridsearch(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Input: X_train and y_train for model training\n",
    "    Output: Best params\n",
    "    \n",
    "    \"\"\"\n",
    "    # Inputting the different parameters obtained from the RandomizedSearchCV\n",
    "    gradient_params = {\"loss\":[\"deviance\", \"exponential\"],\n",
    "                       \"criterion\":[\"friedman_mse\", \"mse\"],\n",
    "                       \"max_features\":[\"log2\",\"sqrt\"],\n",
    "                       \"n_estimators\":[260, 265, 270, 275, 280],\n",
    "                       \"max_depth\":[3, 4, 5],\n",
    "                       \"max_leaf_nodes\":[10, 12, 14, 16]\n",
    "                      }\n",
    "\n",
    "    # Running the best_gradient with GridSearchCV\n",
    "    best_gradient = GridSearchCV(gradient,\n",
    "                                 gradient_params,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=StratifiedKFold(),\n",
    "                                 scoring=\"f1_macro\"\n",
    "                                )\n",
    "    \n",
    "    best_gradient.fit(X_train, y_train)\n",
    "    \n",
    "    best = best_gradient.best_estimator_\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=5, max_features='log2', max_leaf_nodes=14,\n",
       "                           n_estimators=260)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_gridsearch(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_trainer(X_train, X_test, y_train, y_test, loss, criterion, max_features, n_estimators, max_depth, max_leaf_nodes):\n",
    "    \"\"\"\n",
    "    Input: All the inputs for training the GradientBoostingClassifier with GridSearchCV\n",
    "    Output: y_pred\n",
    "    \"\"\"\n",
    "    # Creating the best_gradient \n",
    "    best_gradient = GradientBoostingClassifier(loss=loss, criterion=criterion,\n",
    "                                               max_features=max_features, n_estimators=n_estimators,\n",
    "                                               max_depth=max_depth, max_leaf_nodes=max_leaf_nodes\n",
    "                                              )\n",
    "    \n",
    "    # Model fit\n",
    "    best_gradient.fit(X_train, y_train)\n",
    "    \n",
    "    # Model train\n",
    "    y_pred = best_gradient.predict(X_test)\n",
    "    \n",
    "    # Getting classification report\n",
    "    c_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True, target_names=['Churn', 'Customer']))\n",
    "    \n",
    "    # Getting Kappa score\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, c_report, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gradient, classification_report_gradient, kappa_gradient = gradient_trainer(X_train_scaled,\n",
    "                                                                                   X_test_scaled,\n",
    "                                                                                   y_train,\n",
    "                                                                                   y_test,\n",
    "                                                                                   \"deviance\",\n",
    "                                                                                   \"friedman_mse\",\n",
    "                                                                                   \"log2\",\n",
    "                                                                                   265,\n",
    "                                                                                   5,\n",
    "                                                                                   14\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# adaboost_params = {\"algorithm\":[\"SAMME\", \"SAMME.R\"],\n",
    "#                   \"n_estimators\":randint(low=10, high=200)\n",
    "#                   }\n",
    "\n",
    "# scorers = {\"precision_score\": make_scorer(precision_score, average=\"macro\"),\n",
    "#            \"recall_score\": make_scorer(recall_score, average=\"macro\"),\n",
    "#            \"f1_score\": make_scorer(f1_score, average=\"macro\")\n",
    "#           }\n",
    "\n",
    "# adaboost_search = RandomizedSearchCV(adaboost,\n",
    "#                                      adaboost_params,\n",
    "#                                      n_iter=10,\n",
    "#                                      n_jobs=-1,\n",
    "#                                      cv=10,\n",
    "#                                      scoring=scorers,\n",
    "#                                      refit=False,\n",
    "#                                      random_state=42\n",
    "#                                     )\n",
    "\n",
    "# adaboost_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost_results = pd.DataFrame(adaboost_search.cv_results_)\n",
    "\n",
    "# adaboost_results = adaboost_results[[\"params\", \"mean_test_precision_score\", \"rank_test_precision_score\",\n",
    "#                                      \"mean_test_recall_score\", \"rank_test_recall_score\",\n",
    "#                                      \"mean_test_f1_score\", \"rank_test_f1_score\"]]\n",
    "\n",
    "# adaboost_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost_results[\"params\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_gridsearch(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Input: X_train and y_train for model training\n",
    "    Output: Best params\n",
    "    \n",
    "    \"\"\"\n",
    "    # Inputting the different parameters obtained from the RandomizedSearchCV\n",
    "    adaboost_params = {\"algorithm\":[\"SAMME\", \"SAMME.R\"],\n",
    "                       \"n_estimators\":[155, 158, 160, 161, 164]\n",
    "                      }\n",
    "\n",
    "    # Running the best_gradient with GridSearchCV\n",
    "    best_adaboost = GridSearchCV(adaboost,\n",
    "                                 adaboost_params,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=StratifiedKFold(),\n",
    "                                 scoring=\"f1_macro\"\n",
    "                                )\n",
    "    \n",
    "    best_adaboost.fit(X_train, y_train)\n",
    "    \n",
    "    best = best_adaboost.best_estimator_\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=155)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_gridsearch(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_trainer(X_train, X_test, y_train, y_test, algorithm, n_estimators):\n",
    "    \"\"\"\n",
    "    Input: All the inputs for training the AdaBoostClassifier with GridSearchCV\n",
    "    Output: y_pred\n",
    "    \"\"\"\n",
    "    # Creating the best_adaboost \n",
    "    best_adaboost = AdaBoostClassifier(algorithm=algorithm, n_estimators=n_estimators)\n",
    "    \n",
    "    # Model fit\n",
    "    best_adaboost.fit(X_train, y_train)\n",
    "    \n",
    "    # Model train\n",
    "    y_pred = best_adaboost.predict(X_test)\n",
    "    \n",
    "    # Getting classification report\n",
    "    c_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True, target_names=['Churn', 'Customer']))\n",
    "    \n",
    "    # Getting Kappa score\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, c_report, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_adaboost, classification_report_adaboost, kappa_adaboost = adaboost_trainer(X_train_scaled,\n",
    "                                                                                   X_test_scaled,\n",
    "                                                                                   y_train,\n",
    "                                                                                   y_test,\n",
    "                                                                                   \"SAMME.R\",\n",
    "                                                                                   155\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# RF_params = {\"criterion\":[\"gini\", \"entropy\"],\n",
    "#              \"max_features\":[\"auto\", \"sqrt\", \"log2\"],\n",
    "#              \"class_weight\":[\"balanced\", \"balanced_subsample\"],          \n",
    "#              \"n_estimators\":randint(low=10, high=400),\n",
    "#              \"max_depth\":randint(low=2, high=20),\n",
    "#              \"min_samples_split\":randint(low=2, high=40)\n",
    "#             }\n",
    "\n",
    "# scorers = {\"precision_score\": make_scorer(precision_score, average=\"macro\"),\n",
    "#            \"recall_score\": make_scorer(recall_score, average=\"macro\"),\n",
    "#            \"f1_score\": make_scorer(f1_score, average=\"macro\")\n",
    "#           }\n",
    "\n",
    "# RF_search = RandomizedSearchCV(RF,\n",
    "#                                RF_params,\n",
    "#                                n_iter=10,\n",
    "#                                n_jobs=-1,\n",
    "#                                cv=50,\n",
    "#                                scoring=scorers,\n",
    "#                                refit=False,\n",
    "#                                random_state=42)\n",
    "\n",
    "# RF_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_results = pd.DataFrame(RF_search.cv_results_)\n",
    "\n",
    "# RF_results = RF_results[[\"params\", \"mean_test_precision_score\", \"rank_test_precision_score\",\n",
    "#                          \"mean_test_recall_score\", \"rank_test_recall_score\",\n",
    "#                          \"mean_test_f1_score\", \"rank_test_f1_score\"]]\n",
    "\n",
    "# RF_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_results[\"params\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_gridsearch(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Input: X_train and y_train for model training\n",
    "    Output: Best params\n",
    "    \n",
    "    \"\"\"\n",
    "    # Inputting the different parameters obtained from the RandomizedSearchCV    \n",
    "    RF_params = {\"criterion\":[\"gini\", \"entropy\"],\n",
    "                 \"max_features\":[\"auto\", \"sqrt\", \"log2\"],\n",
    "                 \"class_weight\":[\"balanced\", \"balanced_subsample\"],               \n",
    "                 \"n_estimators\":[390, 395, 400, 405],\n",
    "                 \"max_depth\":[8, 10, 12],\n",
    "                 \"min_samples_split\":[16, 18, 20, 22]\n",
    "                }\n",
    "\n",
    "    # Running the best_gradient with GridSearchCV\n",
    "    best_RF = GridSearchCV(RF,\n",
    "                                 RF_params,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=StratifiedKFold(),\n",
    "                                 scoring=\"f1_macro\"\n",
    "                                )\n",
    "    \n",
    "    best_RF.fit(X_train, y_train)\n",
    "    \n",
    "    best = best_RF.best_estimator_\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
       "                       max_depth=12, min_samples_split=18, n_estimators=390)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_gridsearch(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_trainer(X_train, X_test, y_train, y_test, class_weight, criterion, max_features, max_depth, min_samples_split, n_estimators):\n",
    "    \"\"\"\n",
    "    Input: All the inputs for training the RandomForestClassifier with GridSearchCV\n",
    "    Output: y_pred\n",
    "    \"\"\"\n",
    "    # Creating the best_RF\n",
    "    best_RF = RandomForestClassifier(class_weight=class_weight, criterion=criterion, max_features=max_features,\n",
    "                                     max_depth=max_depth, min_samples_split=min_samples_split, n_estimators=n_estimators)\n",
    "    \n",
    "    # Model fit\n",
    "    best_RF.fit(X_train, y_train)\n",
    "    \n",
    "    # Model train\n",
    "    y_pred = best_RF.predict(X_test)\n",
    "    \n",
    "    # Getting classification report\n",
    "    c_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True, target_names=['Churn', 'Customer']))\n",
    "    \n",
    "    # Getting Kappa score\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    return y_pred, c_report, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF, classification_report_RF, kappa_RF = RF_trainer(X_train_scaled,\n",
    "                                                           X_test_scaled,\n",
    "                                                           y_train,\n",
    "                                                           y_test,\n",
    "                                                           \"balanced\",\n",
    "                                                           \"entropy\",\n",
    "                                                           \"auto\",\n",
    "                                                           12,\n",
    "                                                           18,\n",
    "                                                           390\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_predictions(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Input: Variables for training a model\n",
    "    Output: y_pred for three different models    \n",
    "    \"\"\"\n",
    "    # GradientBoost\n",
    "    gradient.fit(X_train, y_train)\n",
    "    y_pred_initial_gradient = gradient.predict(X_test)\n",
    "    \n",
    "    # AdaBoost\n",
    "    adaboost.fit(X_train, y_train)\n",
    "    y_pred_initial_adaboost = adaboost.predict(X_test)\n",
    "    \n",
    "    # RF\n",
    "    RF.fit(X_train, y_train)\n",
    "    y_pred_initial_RF = RF.predict(X_test)\n",
    "    \n",
    "    return y_pred_initial_gradient, y_pred_initial_adaboost, y_pred_initial_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_initial_gradient, y_pred_initial_adaboost, y_pred_initial_RF = initial_predictions(X_train_scaled, \n",
    "                                                                                          y_train, \n",
    "                                                                                          X_test_scaled\n",
    "                                                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_class_reports(y_test, y_pred1, y_pred2, y_pred3):\n",
    "    \"\"\"\n",
    "    Input: Three y_pred variables of the different models\n",
    "    Output: Classification reports of the models    \n",
    "    \"\"\"\n",
    "    # GradientBoost\n",
    "    initial_classification_gradient = pd.DataFrame(classification_report(y_test, y_pred1, output_dict=True, target_names=['Churn', 'Customer']))\n",
    "    \n",
    "    # AdaBoost\n",
    "    initial_classification_adaboost = pd.DataFrame(classification_report(y_test, y_pred2, output_dict=True, target_names=['Churn', 'Customer']))\n",
    "    \n",
    "    # RF\n",
    "    initial_classification_RF = pd.DataFrame(classification_report(y_test, y_pred3, output_dict=True, target_names=['Churn', 'Customer']))\n",
    "    \n",
    "    return initial_classification_gradient, initial_classification_adaboost, initial_classification_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_class_gradient, first_class_adaboost, first_class_RF = initial_class_reports(y_test,\n",
    "                                                                                  y_pred_initial_gradient,\n",
    "                                                                                  y_pred_initial_adaboost,\n",
    "                                                                                  y_pred_initial_RF\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost\n",
      "Initial:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.923333     0.972190  0.964956     0.947762      0.964353\n",
      "recall       0.852308     0.986479  0.964956     0.919393      0.964956\n",
      "f1-score     0.886400     0.979282  0.964956     0.932841      0.964383\n",
      "support    325.000000  1701.000000  0.964956  2026.000000   2026.000000\n",
      "\n",
      "Modified:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.948052     0.980792  0.975814     0.964422      0.975540\n",
      "recall       0.898462     0.990594  0.975814     0.944528      0.975814\n",
      "f1-score     0.922591     0.985668  0.975814     0.954130      0.975550\n",
      "support    325.000000  1701.000000  0.975814  2026.000000   2026.000000\n",
      "\n",
      "\n",
      "AdaBoost\n",
      "Initial:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.893891     0.972595  0.960513     0.933243      0.959969\n",
      "recall       0.855385     0.980600  0.960513     0.917992      0.960513\n",
      "f1-score     0.874214     0.976581  0.960513     0.925397      0.960160\n",
      "support    325.000000  1701.000000  0.960513  2026.000000   2026.000000\n",
      "\n",
      "Modified:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.897196     0.978299  0.965449     0.937748      0.965289\n",
      "recall       0.886154     0.980600  0.965449     0.933377      0.965449\n",
      "f1-score     0.891641     0.979448  0.965449     0.935544      0.965362\n",
      "support    325.000000  1701.000000  0.965449  2026.000000   2026.000000\n",
      "\n",
      "\n",
      "RandomForest\n",
      "Initial:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.927586     0.967742  0.961994     0.947664      0.961300\n",
      "recall       0.827692     0.987654  0.961994     0.907673      0.961994\n",
      "f1-score     0.874797     0.977597  0.961994     0.926197      0.961106\n",
      "support    325.000000  1701.000000  0.961994  2026.000000   2026.000000\n",
      "\n",
      "Modified:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.846821     0.980952  0.958045     0.913887      0.959436\n",
      "recall       0.901538     0.968842  0.958045     0.935190      0.958045\n",
      "f1-score     0.873323     0.974860  0.958045     0.924091      0.958572\n",
      "support    325.000000  1701.000000  0.958045  2026.000000   2026.000000\n",
      "\n",
      "\n",
      "GradientBoost Kappa: 0.9083\n",
      "AdaBoost Kappa: 0.8711\n",
      "RF Kappa: 0.8482\n"
     ]
    }
   ],
   "source": [
    "# Comparing GradientBoost\n",
    "print(f\"GradientBoost\\nInitial:\\n{first_class_gradient}\\n\")\n",
    "print(f\"Modified:\\n{classification_report_gradient}\\n\\n\")\n",
    "\n",
    "# Comparing AdaBoost\n",
    "print(f\"AdaBoost\\nInitial:\\n{first_class_adaboost}\\n\")\n",
    "print(f\"Modified:\\n{classification_report_adaboost}\\n\\n\")\n",
    "\n",
    "# Comparing RandomForest\n",
    "print(f\"RandomForest\\nInitial:\\n{first_class_RF}\\n\")\n",
    "print(f\"Modified:\\n{classification_report_RF}\\n\\n\")\n",
    "\n",
    "# Looking Kappa\n",
    "print(f\"GradientBoost Kappa: {round(kappa_gradient,4)}\")\n",
    "print(f\"AdaBoost Kappa: {round(kappa_adaboost,4)}\")\n",
    "print(f\"RF Kappa: {round(kappa_RF,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the `classification_report`, we will focus on the **Churn** column. On a first view, we can identify that the best models are the `GradientBoost` and `RandomForest`. Let's dive deep and compare each other:\n",
    "1. **`GradientBoost`**: The `precision` is 0.9480, the `recall` is 0.8984, the `f1-score` is 0.9225 and the `accuracy` is 0.9758.\n",
    "2. **`RandomForest`**: The `precision` is 0.8468, the `recall` is 0.9015, the `f1-score` is 0.8733 and the `accuracy` is 0.9580.\n",
    "\n",
    "Although the `recall` is slightly higher on the `RandomForest`, the precision has a big difference compared to the `GradientBoost`. Also, taking into consideration that the `f1-score` will give us a more accurate reference of which model performs better with a more conservative approach, we can ensure that the **`GradientBoost`** is the best model overall.\n",
    "\n",
    "We can also notice that the **Customer** column is much better on the `GradientBoost` rather than on the `RandomForest`, and, lastly, there is a considerable difference of `cohen_kappa_score` between both models.\n",
    "\n",
    "For further testing, we will only focus with **`GradientBoost`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already did our initial `RandomizedSearchCV` for obtaining the values were we should focus with the `GridSearchCV`, in this case is not neccesary to do that step again.\n",
    "\n",
    "What we will do, is the following:\n",
    "* Split the test size in two different values, **0.1** and **0.3**.\n",
    "* Obtain the best parameters for the gridsearch and see if they are slightly different from the previous model.\n",
    "* Train the models with the new samples.\n",
    "* Compare the new `classification_report` with the previous one and see if there is improvement. **In case that the model improves, for further analysis we will use the best one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10% Test Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10, X_test_10, y_train_10, y_test_10 = train_test_split(X, y, test_size = 0.1, stratify=y ,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "X_train_scaled_10 = scaler.fit_transform(X_train_10)\n",
    "X_test_scaled_10 = scaler.transform(X_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='mse', max_depth=5, max_features='sqrt',\n",
       "                           max_leaf_nodes=16, n_estimators=265)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining best params\n",
    "gradient_gridsearch(X_train_scaled_10, y_train_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing y_pred\n",
    "y_pred_gradient_10, class_report_gradient_10, kappa_gradient_10 = gradient_trainer(X_train_scaled_10,\n",
    "                                                                                   X_test_scaled_10,\n",
    "                                                                                   y_train_10,\n",
    "                                                                                   y_test_10,\n",
    "                                                                                   \"deviance\",\n",
    "                                                                                   \"mse\",\n",
    "                                                                                   \"sqrt\",\n",
    "                                                                                   265,\n",
    "                                                                                   5,\n",
    "                                                                                   16\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30% Test Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_30, X_test_30, y_train_30, y_test_30 = train_test_split(X, y, test_size = 0.3, stratify=y ,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "X_train_scaled_30 = scaler.fit_transform(X_train_30)\n",
    "X_test_scaled_30 = scaler.transform(X_test_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=5, max_features='log2', max_leaf_nodes=14,\n",
       "                           n_estimators=270)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining best params\n",
    "gradient_gridsearch(X_train_scaled_30, y_train_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing y_pred\n",
    "y_pred_gradient_30, class_report_gradient_30, kappa_gradient_30 = gradient_trainer(X_train_scaled_30,\n",
    "                                                                                   X_test_scaled_30,\n",
    "                                                                                   y_train_30,\n",
    "                                                                                   y_test_30,\n",
    "                                                                                   \"deviance\",\n",
    "                                                                                   \"friedman_mse\",\n",
    "                                                                                   \"log2\",\n",
    "                                                                                   270,\n",
    "                                                                                   5,\n",
    "                                                                                   14\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New sample results VS Previous sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost\n",
      "test_size=0.2:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.948052     0.980792  0.975814     0.964422      0.975540\n",
      "recall       0.898462     0.990594  0.975814     0.944528      0.975814\n",
      "f1-score     0.922591     0.985668  0.975814     0.954130      0.975550\n",
      "support    325.000000  1701.000000  0.975814  2026.000000   2026.000000\n",
      "\n",
      "test_size=0.1:\n",
      "                Churn    Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.920732    0.985866  0.975321     0.953299      0.975385\n",
      "recall       0.926380    0.984706  0.975321     0.955543      0.975321\n",
      "f1-score     0.923547    0.985285  0.975321     0.954416      0.975351\n",
      "support    163.000000  850.000000  0.975321  1013.000000   1013.000000\n",
      "\n",
      "test_size=0.3:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.939732     0.974141  0.969069     0.956937      0.968616\n",
      "recall       0.862705     0.989416  0.969069     0.926060      0.969069\n",
      "f1-score     0.899573     0.981719  0.969069     0.940646      0.968528\n",
      "support    488.000000  2551.000000  0.969069  3039.000000   3039.000000\n",
      "\n",
      "\n",
      "test_size=0.2 Kappa: 0.9083\n",
      "test_size=0.1 Kappa: 0.9088\n",
      "test_size=0.3 Kappa: 0.8813\n"
     ]
    }
   ],
   "source": [
    "# Comparing GradientBoost\n",
    "print(f\"GradientBoost\\ntest_size=0.2:\\n{classification_report_gradient}\\n\")\n",
    "print(f\"test_size=0.1:\\n{class_report_gradient_10}\\n\")\n",
    "print(f\"test_size=0.3:\\n{class_report_gradient_30}\\n\\n\")\n",
    "\n",
    "print(f\"test_size=0.2 Kappa: {round(kappa_gradient,4)}\")\n",
    "print(f\"test_size=0.1 Kappa: {round(kappa_gradient_10,4)}\")\n",
    "print(f\"test_size=0.3 Kappa: {round(kappa_gradient_30,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`test_size` reduced**\n",
    "\n",
    "Looking on the *Churn* customers, when the `test_size` is reduced, the `recall` improves and the `precision` goes worse.\n",
    "\n",
    "The `recall` for the churned customers improved considerable, above the 0.9, and although that the overall accuracy dropped a little bit, the `f1-score` increased, which offers a more stable model.\n",
    "\n",
    "\n",
    "* **`test_size` amplified**\n",
    "\n",
    "When we increase the `test_size`, the `recall` for the *Churn* customers goes down, but same happens with the `precision` and, per se, with the `f1-score`. We can conclude that, when the sample increases, results go worse.\n",
    "\n",
    "\n",
    "* **conclusions**\n",
    "\n",
    "For the conclusions, as we discussed that, in some aspects, a reduced `test_size` improved certain parts of the model, it also happens that others drop. But here are the main aspects why **`test_size=0.1`** is better:\n",
    "1. The percentage of `recall` is much better on that scenario.\n",
    "2. Although `precision` is worse, what we are losing is less that what we are winning (overall `f1-score` improved).\n",
    "3. `cohen_kappa_score` shows a tiny improvement with the reduced sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the measures take into consideration the `macro avg` instead of the `weighted avg`, it would be a good idea to see how the models performs with **resampling**. In this case, there will be two kind of resamples:\n",
    "\n",
    "1. **Over Sampling**: Fake data will be created for the train set using the `SMOTE` method, increasing the size of the minority class as maximum as possible.\n",
    "2. **Under Sampling**: Original data will be removed for the train set using `NearMiss` method, reducing the majority class to the same amount as the minority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({1.0: 6799, 0.0: 1302})\n",
      "Resample dataset shape: Counter({0.0: 6799, 1.0: 6799})\n"
     ]
    }
   ],
   "source": [
    "# initializing SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# checking that the amount is the same for each value\n",
    "\n",
    "print('Original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape:', Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "X_train_scaled_smote = scaler.fit_transform(X_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='mse', loss='exponential', max_depth=4,\n",
       "                           max_features='sqrt', max_leaf_nodes=16,\n",
       "                           n_estimators=265)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining best params\n",
    "gradient_gridsearch(X_train_scaled_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing y_pred\n",
    "y_pred_gradient_smote, class_report_gradient_smote, kappa_gradient_smote = gradient_trainer(X_train_scaled_smote,\n",
    "                                                                                            X_test_scaled,\n",
    "                                                                                            y_train_smote,\n",
    "                                                                                            y_test,\n",
    "                                                                                            \"exponential\",\n",
    "                                                                                            \"mse\",\n",
    "                                                                                            \"sqrt\",\n",
    "                                                                                            265,\n",
    "                                                                                            4,\n",
    "                                                                                            16\n",
    "                                                                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({1.0: 6799, 0.0: 1302})\n",
      "Resample dataset shape: Counter({0.0: 1302, 1.0: 1302})\n"
     ]
    }
   ],
   "source": [
    "# initializing NearMiss\n",
    "\n",
    "nm = NearMiss()\n",
    "\n",
    "X_train_nm, y_train_nm = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "# checking that the amount is the same for each value\n",
    "\n",
    "print('Original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape:', Counter(y_train_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "X_train_scaled_nm = scaler.fit_transform(X_train_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='mse', max_depth=4, max_features='log2',\n",
       "                           max_leaf_nodes=14, n_estimators=280)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining best params\n",
    "gradient_gridsearch(X_train_scaled_nm, y_train_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing y_pred\n",
    "y_pred_gradient_nm, class_report_gradient_nm, kappa_gradient_nm = gradient_trainer(X_train_scaled_nm,\n",
    "                                                                                   X_test_scaled,\n",
    "                                                                                   y_train_nm,\n",
    "                                                                                   y_test,\n",
    "                                                                                   \"deviance\",\n",
    "                                                                                   \"mse\",\n",
    "                                                                                   \"log2\",\n",
    "                                                                                   280,\n",
    "                                                                                   4,\n",
    "                                                                                   14\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost\n",
      "test_size=0.1:\n",
      "                Churn    Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.920732    0.985866  0.975321     0.953299      0.975385\n",
      "recall       0.926380    0.984706  0.975321     0.955543      0.975321\n",
      "f1-score     0.923547    0.985285  0.975321     0.954416      0.975351\n",
      "support    163.000000  850.000000  0.975321  1013.000000   1013.000000\n",
      "\n",
      "Over Sampling:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.187104     1.000000   0.30306     0.593552      0.869600\n",
      "recall       1.000000     0.169900   0.30306     0.584950      0.303060\n",
      "f1-score     0.315228     0.290452   0.30306     0.302840      0.294427\n",
      "support    325.000000  1701.000000   0.30306  2026.000000   2026.000000\n",
      "\n",
      "Under Sampling:\n",
      "                Churn     Customer  accuracy    macro avg  weighted avg\n",
      "precision    0.269807     0.933150  0.627345     0.601479      0.826740\n",
      "recall       0.775385     0.599059  0.627345     0.687222      0.627345\n",
      "f1-score     0.400318     0.729681  0.627345     0.565000      0.676847\n",
      "support    325.000000  1701.000000  0.627345  2026.000000   2026.000000\n",
      "\n",
      "\n",
      "test_size=0.1 Kappa: 0.9088\n",
      "Over Sampling Kappa: 0.0616\n",
      "Under Sampling Kappa: 0.213\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(f\"GradientBoost\\ntest_size=0.1:\\n{class_report_gradient_10}\\n\")\n",
    "print(f\"Over Sampling:\\n{class_report_gradient_smote}\\n\")\n",
    "print(f\"Under Sampling:\\n{class_report_gradient_nm}\\n\\n\")\n",
    "\n",
    "print(f\"test_size=0.1 Kappa: {round(kappa_gradient_10,4)}\")\n",
    "print(f\"Over Sampling Kappa: {round(kappa_gradient_smote,4)}\")\n",
    "print(f\"Under Sampling Kappa: {round(kappa_gradient_nm,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the different results, we can conclude that neither **Over Sampling** or **Under Sampling** provides better results (or barely similar) to the previous ones. \n",
    "\n",
    "Although the `recall` for the *Churn* customers is perfect with the over sampling, the rest of variables are really bad, causing that the model has a very low `accuracy` and an even lower `cohen_kappa_score`.\n",
    "\n",
    "On the other hand, the under sampling lowered the values mentioned on the previous paragraph, but not as aggresively as the over sampling does. Although, the model is still pretty bad and we will not consider it.\n",
    "\n",
    "Last, but not least, we remain with the **GradientBoost | test_size = 0.1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
